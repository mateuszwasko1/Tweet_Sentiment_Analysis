{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8001d121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tweet emotion  score\n",
      "0     @xandraaa5 @amayaallyn6 shut up hashtags are c...   anger  0.562\n",
      "1     it makes me so fucking irate jesus. nobody is ...   anger  0.750\n",
      "2            Lol Adam the Bull with his fake outrage...   anger  0.417\n",
      "3     @THATSSHAWTYLO passed away early this morning ...   anger  0.354\n",
      "4     @Kristiann1125 lol wow i was gonna say really?...   anger  0.438\n",
      "...                                                 ...     ...    ...\n",
      "7097  Watch this amazing live.ly broadcast by @kana_...     joy  0.558\n",
      "7098  Watching @melissamccarthy in #Spy she's one of...     joy  0.780\n",
      "7099                            Could not be happier!!      joy  0.885\n",
      "7100  @strictlysimilak something about English spark...     joy  0.360\n",
      "7101  and I think some of our most spiritually weigh...     joy  0.440\n",
      "\n",
      "[7102 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from project_name.preprocessing.baseline_preprocessing import BaselinePreprocessor\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=True)\n",
    "\n",
    "df = pd.read_json(\"../data/raw/training_merged.json\", orient=\"records\", lines=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a48c632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet emotion  score  label\n",
      "0  @xandraaa5 @amayaallyn6 shut up hashtags are c...   anger  0.562      0\n",
      "1  it makes me so fucking irate jesus. nobody is ...   anger  0.750      0\n",
      "2         Lol Adam the Bull with his fake outrage...   anger  0.417      0\n",
      "3  @THATSSHAWTYLO passed away early this morning ...   anger  0.354      0\n",
      "4  @Kristiann1125 lol wow i was gonna say really?...   anger  0.438      0\n"
     ]
    }
   ],
   "source": [
    "label_map = {label: idx for idx, label in enumerate(df[\"emotion\"].unique())}\n",
    "df[\"label\"] = df[\"emotion\"].map(label_map)\n",
    "\n",
    "NUM_CLASSES = len(label_map)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8612a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def bertweet_preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@\\w+', '@USER', text)\n",
    "    text = re.sub(r'http\\S+', 'HTTPURL', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d05bae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"tweet\"] = df[\"tweet\"].apply(bertweet_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc66297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df[[\"tweet\", \"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1e58c1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad520d78e6384013baf5ff8eef87640f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tweet', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 7102\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"tweet\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb16b9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mvinai/bertweet-base\u001b[39m\u001b[33m\"\u001b[39m, use_fast=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mvinai/bertweet-base\u001b[39m\u001b[33m\"\u001b[39m, num_labels=NUM_CLASSES)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m trainer = Trainer(\n\u001b[32m     16\u001b[39m     model=model,\n\u001b[32m     17\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     tokenizer=tokenizer\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m trainer.train()\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Programming/.local/share/virtualenvs/Applied-ML-Template-1-YC0RMAhE/lib/python3.11/site-packages/huggingface_hub/file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 542.51 MB. The target location /Users/Programming/.cache/huggingface/hub/models--vinai--bertweet-base/blobs only has 38.53 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc98d98cb16347d4bb845e3d41874128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  33%|###3      | 273M/815M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=NUM_CLASSES)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Applied-ML-Template-1-YC0RMAhE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
